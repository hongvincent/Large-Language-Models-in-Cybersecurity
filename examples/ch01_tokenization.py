"""
Chapter 1: Tokenization ì‹¤ìŠµ
LLMì˜ í† í°í™” ê³¼ì •ì„ ì´í•´í•˜ê¸° ìœ„í•œ ì˜ˆì œ
"""

import os
from dotenv import load_dotenv
import openai

# Load environment variables
load_dotenv()

# Initialize OpenAI client
client = openai.OpenAI(api_key=os.getenv("OPENAI_API_KEY"))

def analyze_tokenization(text: str):
    """í…ìŠ¤íŠ¸ì˜ í† í°í™” ë¶„ì„"""
    print(f"\n{'='*60}")
    print(f"Original Text: {text}")
    print(f"{'='*60}")

    # GPT-5.1ì„ ì‚¬ìš©í•˜ì—¬ í† í° ë¶„ì„
    response = client.chat.completions.create(
        model="gpt-5.1-chat-latest",
        messages=[
            {
                "role": "user",
                "content": f"""Analyze the tokenization of this text: "{text}"

Explain:
1. How many tokens would this be?
2. How would it be split into tokens?
3. Why is tokenization important for LLMs?"""
            }
        ]
    )

    print(f"\nTokenization Analysis:")
    print(response.choices[0].message.content)
    print(f"\n{'='*60}\n")


def main():
    """ë©”ì¸ í•¨ìˆ˜"""
    print("\nğŸ” Chapter 1: Tokenization ì‹¤ìŠµ\n")

    # ë‹¤ì–‘í•œ í…ìŠ¤íŠ¸ë¡œ í† í°í™” í…ŒìŠ¤íŠ¸
    test_texts = [
        "Hello, world!",
        "Large Language Models in Cybersecurity",
        "í•œê¸€ë„ í† í°í™”ê°€ ì–´ë–»ê²Œ ë ê¹Œìš”?",
        "Code: print('Hello')",
        "SQL injection: ' OR '1'='1",
    ]

    for text in test_texts:
        analyze_tokenization(text)


if __name__ == "__main__":
    main()
